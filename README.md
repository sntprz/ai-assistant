# Asistente Inteligente de Conocimiento Corporativo ‚Äî MVP 2 Semanas

## Resumen Ejecutivo

**Nombre del proyecto:** Asistente inteligente de conocimiento corporativo ‚Äî MVP 2 semanas (RAG + 2 agents + Agent Manager)

### Visi√≥n Breve
Construir en 10 d√≠as laborables un MVP demostrable que permita a un usuario subir documentos, consultar sobre ellos con respuestas basadas en evidencia (RAG), y disponer de dos agentes orquestados por un Agent Manager capaz de ejecutar agentes de forma individual o encadenada (chain). El objetivo es validar el flujo end-to-end: ingesti√≥n ‚Üí indexado ‚Üí retrieval ‚Üí generaci√≥n ‚Üí orquestaci√≥n de agentes ‚Üí trazabilidad y feedback. Todo debe ser reproducible localmente (Docker) y documentado para continuar su evoluci√≥n.

### Por Qu√© es Importante

- Resuelve un problema real y transversal: encontrar y estructurar conocimiento dentro de documentos largos y poco estructurados.
- Ense√±a las habilidades pr√°cticas m√°s demandadas de un AI Engineer orientado a producto: integraci√≥n con modelos, indexaci√≥n vectorial, APIs, UI, orquestaci√≥n (agents), evaluaci√≥n y pr√°cticas m√≠nimas de MLOps.
- Entrega artefactos accionables (c√≥digo reproducible, demo, m√©tricas y backlog) que pueden escalarse en sprints posteriores.

### Valor Entregable en 2 Semanas

- Demo funcional (UI + API) que responde preguntas con evidencia y permite invocar dos agentes: Summarizer & Action-Extractor y Email Drafter.
- Agent Manager que registra, selecciona y encadena agentes, con trazabilidad y m√©tricas b√°sicas.
- Corpus inicial (~20 documentos) y pipeline reproducible para extracci√≥n, chunking, embeddings y FAISS local.
- Scripts de evaluaci√≥n y criterios cuantitativos que demuestran el rendimiento m√≠nimo aceptable.
- C√≥digo dockerizado y documentaci√≥n para reproducir la demo en otra m√°quina.

## Alcance (Detallado)

### 1) Entregables Funcionales (qu√© S√ç est√° incluido)

#### Datos y Pipeline
- Corpus inicial de ~20 documentos (formatos txt/md; PDFs opcionales) con metadata b√°sica (doc_id, title, author, date, category).
- Scripts reproducibles de ingesti√≥n: extracci√≥n de texto ‚Üí limpieza ‚Üí chunking (p. ej. 500 tokens por chunk) ‚Üí generaci√≥n de embeddings ‚Üí indexado local en FAISS.
- Repository con estructura, requirements.txt/pyproject.toml, y Dockerfile.

#### B√∫squeda y RAG
- Recuperador top-K sobre FAISS (funci√≥n `retrieve(query, k)`).
- Componente RAG (funci√≥n `answer_query(query)`) que:
  - Recupera top-K, construye prompt con contexto y metadatos.
  - Llama a un LLM (API o local) para generar respuesta.
  - Devuelve: `{ answer: str, sources: [{doc_id, chunk_id, score}], latency_ms }`.

#### Agentes y Agent Manager
- **Agent A (Summarizer & Action-Extractor)**: recibe doc_id o text y devuelve `{ summary: str, actions: [str], sources: [...] }`.
- **Agent B (Email Drafter)**: recibe `{ summary, actions, recipient_name, tone }` y devuelve `{ subject: str, body: str, suggested_edits: [str] }`.
- **Agent Manager (FastAPI)**:
  - **Endpoints:**
    - `GET /agents` ‚Äî lista de agentes disponibles y metadatos.
    - `POST /run_agent/{name}` ‚Äî ejecutar un agente con payload JSON.
    - `POST /run_chain` ‚Äî ejecutar una cadena predefinida (ej.: Summarizer ‚Üí Email Drafter).
    - `GET /traces/{trace_id}` ‚Äî recuperar trazas de ejecuci√≥n.
  - Registro de trazas (persistencia en SQLite o JSON): inputs, outputs, pasajes usados, tiempos, feedback.
  - Capacidad de chaining (pipeline de agentes) con manejo b√°sico de errores y retries limitados.

#### UI / UX
- **Streamlit app** con:
  - Upload de documentos y listado / visualizaci√≥n.
  - Chat/QA: campo de consulta, muestra de respuesta + fuentes.
  - Panel de Agentes: botones Run Summarizer, Run Email Drafter, Run Chain.
  - Panel de trazas: ver inputs, pasajes recuperados, outputs y bot√≥n de feedback üëç/üëé.
  - Dashboard de m√©tricas b√°sicas (latencia, thumbs-up ratio, success_rate por agente).

#### Observabilidad y Evaluaci√≥n
- Logging de eventos (queries, agent runs, feedback) persistido.
- **Scripts de evaluaci√≥n:**
  - Set de 20 Q/A para evaluar RAG.
  - 10 casos de prueba para cada agente (Agent A y B) con criterios de aceptaci√≥n.
  - Reporte autom√°tico (CSV/JSON) con m√©tricas: % respuestas con cita, latency p50/p95, thumbs-up ratio.

#### Empaquetado y Reproducibilidad
- Dockerfile que construye una imagen con backend + UI ejecutables localmente.
- README y playbook: instrucciones para levantar local, c√≥mo a√±adir docs y ejecutar demo.
- Issues y backlog priorizado para siguientes pasos (Pinecone, deploy cloud, agents adicionales, fine-tuning).

### 2) Fuera de Alcance (qu√© NO est√° incluido en este sprint)

- Despliegue en Kubernetes, infra como c√≥digo (Terraform) o cloud managed vector DB (a excepci√≥n de pruebas opcionales si hay tiempo).
- Integraci√≥n con servicios de correo real o ticketing (solo mocks/simulaci√≥n).
- Fine-tuning a escala (solo experimentos ligeros si hay tiempo).
- Hardening de seguridad empresarial, auditor√≠a legal o cumplimiento GDPR completo (se aplican pr√°cticas m√≠nimas: tokens, no exponer datos reales).

### 3) Suposiciones Operativas (qu√© damos por hecho)

- Tendr√°s acceso a una m√°quina con Python 3.9+, Docker, y espacio suficiente para instalar dependencias de modelos (o una API key de OpenAI/Azure si prefieres external LLM).
- No se utilizar√°n datos sensibles reales en el corpus inicial; si decides usar datos reales, t√∫ aseguras permisos y anonimizaci√≥n.
- Las latencias objetivo (ej. p95 < 3s) son realistas solo con modelos/modo API peque√±os; en local, depende del hardware.
- El alcance de los agentes est√° limitado a la generaci√≥n de texto y estructuras JSON; integraci√≥n con sistemas externos queda para sprints posteriores.

## Objetivos (Extensos, Medibles y Ordenados)

### 1) Objetivos del Proyecto (High Level)

- **Entregar un MVP funcional**: Desplegar localmente (Docker) una aplicaci√≥n que permita upload de documentos, QA con evidencia y ejecuci√≥n de agentes (A y B) orquestados por un Agent Manager.
- **Proveer trazabilidad**: Cada ejecuci√≥n de agente y consulta debe quedar registrada con inputs, pasajes recuperados, outputs, tiempos y feedback.
- **Habilitar evaluaci√≥n reproducible**: Scripts y data para correr benchs autom√°ticos y cuantificar desempe√±o.
- **Documentar y entregar backlog**: README, playbook de operaci√≥n m√≠nima y lista priorizada de mejoras.

### 2) Objetivos Operacionales y M√©tricas (SMART)

#### MVP Operativo Localmente
- **Meta**: App levantada y accesible en localhost con Docker.
- **√âxito**: ‚úÖ `docker run` arranca backend + UI y demo reproducible en ‚â§ 5 min.

#### Agent A: Summarizer & Action-Extractor
- **Meta**: Entregar summary (1‚Äì3 frases) y lista de actions (0‚Äì10 items) con fuentes.
- **Medida**: En conjunto de pruebas (n=10), en ‚â• 8/10 casos se considera output aceptable por criterio humano (claridad + relevancia).
- **KPI adicional**: % de outputs con al menos una fuente citada ‚â• 90%.

#### Agent B: Email Drafter
- **Meta**: Generar email profesional (subject + body) que incluya contexto y acciones.
- **Medida**: En n=10 casos, ‚â• 8/10 emails considerados utilizables sin grandes correcciones (evaluaci√≥n humana).
- **KPI adicional**: 80% de emails deben mencionar al menos 1 acci√≥n extra√≠da.

#### Chain Summarizer ‚Üí Email
- **Meta**: Ejecutar chain desde UI y obtener email final en ‚â§ 5 minutos por usuario novel.
- **Medida**: Demo reproducible; test end-to-end exitoso en 5 ejecuciones consecutivas.

#### RAG (QA con Evidencia)
- **Meta**: Responder preguntas citando pasajes.
- **Medida**: % respuestas con cita correcta (la cita contiene la informaci√≥n relevante) ‚â• 80% en set de 20 Q/A.

#### M√©tricas de Rendimiento
- **Latency** (target realista en local con modelos peque√±os/API): p50 ‚â§ 500 ms para retrieval; p95 ‚â§ 3 s para RAG completo. (Si usas API cloud, documentar facturaci√≥n estimada).
- **Feedback positive ratio**: ‚â• 70% thumbs-up en pruebas internas (n‚â•30 interacciones).

#### Trazabilidad y Logging
- **Meta**: Cada run produce un registro con campos m√≠nimos (see secci√≥n DB schema).
- **Medida**: 100% de ejecuciones (en prueba) quedan registradas y consultables.

### 3) Objetivos de Aprendizaje (Personales y Concretos)

Al completar el sprint deber√≠as ser capaz de:

- Explicar en detalle y ejecutar el flujo RAG: chunking ‚Üí embeddings ‚Üí indexado ‚Üí retrieval ‚Üí prompt design ‚Üí generaci√≥n con evidencia.
- Construir y desplegar una API que expone agentes y orquesta chains (conceptos de agent manager).
- Dise√±ar prompts efectivos para extracci√≥n de acciones y generaci√≥n de correos.
- Implementar y analizar m√©tricas b√°sicas de evaluaci√≥n (precision-like, coverage, feedback).
- Dockerizar una aplicaci√≥n y preparar instrucciones para correrla reproduciblemente.

### 4) Definition of Done (DoD) ‚Äî Criterios de Aceptaci√≥n T√©cnicos

El sprint se considerar√° completado si TODAS estas condiciones se cumplen:

- [x] **C√≥digo**: Repo con estructura clara, requirements.txt o pyproject.toml, y Dockerfile.
- [x] **Ejecuci√≥n**: `docker build` + `docker run` levanta la UI y backend en localhost.
- [x] **Funcionalidad RAG**: Endpoint `/query` o la UI responde preguntas y devuelve `{answer, sources}`.
- [x] **Agentes**:
  - `POST /run_agent/summarizer` y `POST /run_agent/email` funcionan con payloads documentados.
  - `POST /run_chain` devuelve resultado completo con trazas.
- [x] **UI**: Streamlit con upload, QA, panel agentes, trazas y feedback.
- [x] **Persistencia**: Logs/trazas almacenados en SQLite/JSON y consultables por trace_id.
- [x] **Evaluaci√≥n**: Scripts que generan reporte con m√©tricas (Q/A coverage, agent success counts).
- [x] **Documentaci√≥n**: README con pasos para reproducir demo y playbook de operaci√≥n (c√≥mo a√±adir docs, c√≥mo evaluar).
- [x] **Backlog**: Issues priorizados para mejoras cr√≠ticas (despliegue cloud, Pinecone, retries avanzados, etc.).
- [x] **Demo**: Grabaci√≥n o script para demo que ejecuta chain y muestra resultados.

## Especificaciones T√©cnicas

### A. Esquemas de Datos / Formatos

## A. Esquemas de Datos / Formatos

### Documento (entrada)

- **doc_id** (str, √∫nico)
- **title** (str)
- **author** (str / opcional)
- **date** (ISO 8601 / opcional)
- **category** (str; e.g., "HR", "IT", "Policy")
- **content** (str) ‚Äî texto extra√≠do

### Chunk / Passage (generado)

- **chunk_id** (str) ‚Äî {doc_id}#chunk_{n}
- **doc_id** (str)
- **text** (str)
- **start_char** (int)
- **end_char** (int)
- **embedding** (float[]) ‚Äî vector embedding

### RAG Response

```json
{ 
  "answer": str, 
  "sources": [ 
    { 
      "doc_id": str, 
      "chunk_id": str, 
      "score": float 
    } 
  ], 
  "latency_ms": int 
}
```

### Agent A Output (Summarizer)

```json
{ 
  "summary": str, 
  "actions": [ 
    { 
      "id": int, 
      "action": str, 
      "confidence": float, 
      "sources": [...] 
    } 
  ], 
  "sources": [...] 
}
```

### Agent B Output (Email Drafter)

```json
{ 
  "subject": str, 
  "body": str, 
  "suggested_edits": [str], 
  "sources": [...] 
}
```

### Trace Record (persistencia)

- **trace_id** (uuid)
- **timestamp** (ISO)
- **type** ("query" | "agent_run" | "chain")
- **user_id** (optional)
- **input** (JSON)
- **retrieved_passages** ([{doc_id, chunk_id, score}])
- **agent_outputs** (JSON)
- **latency_ms** (int)
- **feedback** (null | "up" | "down")
- **notes** (optional)

## B. Endpoints API (FastAPI Minimal Spec)

- `GET /health` ‚Äî devuelve status.
- `POST /ingest` ‚Äî payload `{ doc_id, title, content, metadata }` ‚Üí ingesta + chunking + embeddings + indexado. Respuesta: `{ ok: true, doc_id }`.
- `GET /documents` ‚Äî lista docs.
- `GET /documents/{doc_id}` ‚Äî obtiene doc y chunks.
- `POST /query` ‚Äî payload `{ query: str, top_k: int (opt) }` ‚Üí devuelve RAG response.
- `GET /agents` ‚Äî listado agentes `{ name, description, inputs_schema }`.
- `POST /run_agent/{name}` ‚Äî ejecutar agente, payload depende del agente; devuelve trace_id.
- `POST /run_chain` ‚Äî payload `{ chain: "summarize_email", doc_id, recipient, tone }` ‚Üí devuelve trace_id y final outputs.
- `GET /traces/{trace_id}` ‚Äî recuperar trace.
- `POST /feedback/{trace_id}` ‚Äî `{ feedback: "up"|"down", comment: str }`.

*(Cada endpoint debe validar payload y devolver c√≥digos HTTP est√°ndar.)*

## C. Prompt Design (esquemas, no verbatim para evitar sesgo)

### RAG Prompt Template (answer_query)

**Context:** "You are an assistant. Use the following excerpts to answer the user question. Provide a concise answer and reference the excerpts used."

Insert top-K passages each with `[[source: doc_id#chunk_id]] TEXT`.

**Instruction:** "Answer in max N tokens. Start with the answer, then a section Sources: listing doc_id#chunk_id and short excerpt if necessary."

### Agent A Prompt (summarizer)

**Provide:** title, relevant passages (top-K), user instruction: "Create a short (3‚Äì4 sentence) summary and extract a list of actionable items (imperative sentences). For each action include reference(s) to sources. Output as JSON: `{ summary: str, actions: [{action_id, text, sources: [] }] }`."

### Agent B Prompt (email drafter)

**Inputs:** `{ summary, actions }`, plus recipient_name, tone and optional subject_hint.

**Instruction:** "Write a professional email with subject and body. Include context (1‚Äì2 lines), bullet points for actions, and a polite closing line. Keep email length under X words. Output as JSON `{ subject, body }`."

*(Durante el sprint debes iterar prompts y registrar resultados en un peque√±o experimento para elegir la mejor plantilla.)*

## D. M√©tricas y Definici√≥n Operacional (precisas)

- **Coverage de cita (RAG):** % de respuestas que contienen al menos una fuente entre las top-K recuperadas. F√≥rmula: `count(responses_with_source) / total_responses`. **Objetivo:** ‚â• 80%.

- **Agent A success rate:** % casos (de 10) evaluados manualmente como "summary + actions aceptables". **Objetivo:** ‚â• 80% (8/10).

- **Agent B success rate:** % emails (de 10) aceptables sin grandes cambios. **Objetivo:** ‚â• 80%.

- **Latency retrieval:** p50, p95 en ms. **Objetivo:** retrieval p95 ‚â§ 500 ms (local con embeddings cached).

- **Latency RAG end-to-end:** p50/p95. **Objetivo:** p95 ‚â§ 3 s (depende HW/API).

- **Feedback positive ratio:** thumbs_up / total_feedbacks. **Objetivo:** ‚â• 70% durante pruebas internas.

- **Error rate:** % requests con error HTTP 5xx. **Objetivo:** < 1% en pruebas.

## E. Casos de Prueba M√≠nimos (para evaluaci√≥n autom√°tica/manual)

### RAG (20 queries)

- 10 preguntas factuales (existente en documents).
- 10 preguntas formadas por contexto (requieren sintetizar m√∫ltiples chunks).
- **M√©trica:** coverage de cita y evaluaci√≥n humana por utilidad.

### Agent A (10 casos)

- 5 docs con procedimientos (extraer actions concretos).
- 3 docs narrativos (resumir y extraer acciones impl√≠citas).
- 2 docs corta/no relevante (expectativa: indicar "No suficiente info").

### Agent B (10 casos)

- Entradas con summary + 3‚Äì5 actions ‚Üí generar email con subject y bullets.
- Variaci√≥n en tone (formal, informal).
- Cada caso tendr√° un archivo con input.json y expected_heuristics para facilitar revisi√≥n.

## Plan de Evaluaci√≥n y Ciclo de Mejora (Evaluation-Driven Development)

1. **Before start:** definir m√©tricas (ya las tienes), crear datasets de evaluaci√≥n (Q/A + agent tests).

2. **Durante el sprint:** ejecutar benchs cada vez que cambies prompt o chunking; documentar cambios (W&B o simple CSV).

3. **Post-implementaci√≥n:** correr evaluaci√≥n completa y generar reporte con: coverage, latencias, success rates, ejemplos de failure.

4. **Retro y priorizaci√≥n:** con resultados, crear backlog de mejoras (ej.: mejorar chunking, cambiar modelo de embeddings, ajustar prompts, a√±adir verificaci√≥n humana).

5. **Loop:** priorizar top-3 mejoras para siguiente sprint de 1‚Äì2 semanas.

## Riesgos, Impacto y Mitigaciones (completo)

### Riesgo: No disponer de API Key para LLM o coste demasiado alto
- **Impacto:** retraso o latencias mayores.
- **Mitigaci√≥n:** usar modelos locales peque√±os (HF), reducir tama√±o del corpus, usar caching agresivo.

### Riesgo: Hallucinations (respuestas incorrectas) en agentes
- **Impacto:** outputs no confiables.
- **Mitigaci√≥n:** exigir siempre sources en outputs; si confidence baja, marcar para revisi√≥n humana; mostrar frases de evidencia completas.

### Riesgo: FAISS/embeddings consumen mucha RAM en m√°quina limitada
- **Impacto:** OOM y fallos.
- **Mitigaci√≥n:** usar corpus reducido, embeddings de dimensi√≥n menor, persistir index en disco o usar Pinecone si hay tiempo.

### Riesgo: Chain largo produce latencias altas
- **Impacto:** mala UX.
- **Mitigaci√≥n:** ejecutar agents de forma as√≠ncrona con feedback de estado; optimizar prompt y caching.

### Riesgo: Errores en parsing (PDFs) generan mala indexaci√≥n
- **Mitigaci√≥n:** empezar con txt/markdown; agregar soporte PDF si sobra tiempo.

### Riesgo: Requisitos de seguridad/compliance si se usa data real
- **Mitigaci√≥n:** anonimizar datos y no subir informaci√≥n sensible en el sprint; documentar pol√≠ticas.

## Runbook M√≠nimo (qu√© hacer si algo falla durante demo)

### Backend no responde:
- Comprobar contenedor Docker `docker ps`; logs `docker logs <container>`.
- Si proceso crashed, ver stack trace y reiniciar `docker restart`.

### Index no encontrado / FAISS error:
- Ejecutar `python scripts/ingest.py --reindex` para regenerar index.
- Ver espacio disco/RAM.

### LLM API rate limit / 401:
- Verificar variable de entorno `OPENAI_API_KEY`.
- Si hay rate limit, activar modo de fallback a modelo local o mostrar mensaje al usuario.

### Agent chain falla:
- Consultar trace `GET /traces/{trace_id}`.
- Si error por timeout en LLM, reintentar con modo as√≠ncrono o fallback (shorter prompt).

### Demostraci√≥n en fallo:
- Tener v√≠deo pre-grabado (fallback) y explicar issues.